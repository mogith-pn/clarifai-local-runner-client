import os
import sys

import concurrent

sys.path.append(os.path.dirname(__file__))
from openai_server_starter import OpenAI_APIServer, analyze_multiple_streams, OpenAI_APIServer_Watchdog

##################
import json
from math import ceil
from typing import Any, Dict, Iterator, List

from clarifai.runners.models.model_builder import ModelBuilder
from clarifai.runners.models.openai_class import OpenAIModelClass
from clarifai.utils.logging import logger
from clarifai.runners.utils.data_types import (Image, Video, Audio)
from clarifai.runners.utils.data_utils import Param
from clarifai.runners.utils.openai_convertor import build_openai_messages
from openai import OpenAI


def build_messages(
  prompt: str = "", 
  system_prompt: str = "", 
  images: List[Image] = [],
  audios: List[Audio] = [],
  videos: List[Video] = [],
  chat_history: List[Dict] = [],
  **kwargs
) -> List[Dict]:
  """Construct OpenAI-compatible messages from input components."""
    
  openai_messages = build_openai_messages(
    prompt=prompt,
    images=images,
    audios=audios,
    videos=videos,
    messages=chat_history
  )
  
  if system_prompt:
    openai_messages = [{
        "role": "system",
        "content": system_prompt
    }] + openai_messages

  return openai_messages



class MyRunner(OpenAIModelClass):
  """
  A custom runner that integrates with the Clarifai platform and uses Server inference
  to process inputs, including text and images.
  """
  client = True
  model = True
  
  def _load_default_system_prompt(self):
    """Load the default system prompt from file."""
    try:
      # Look for the default system prompt file in the same directory as the model
      prompt_file = os.path.join(os.path.dirname(__file__), "default_system_prompt.txt")
      with open(prompt_file, 'r', encoding='utf-8') as f:
        return f.read().strip()
    except Exception as e:
      logger.warning(f"Could not load default system prompt from file: {e}")
      # Fallback to hardcoded prompt
      return "You are MM-Poly-8B (not ChatGPT, etc.), a helpful multimodal AI assistant developed by Clarifai researchers (not OpenAI or anyone else). You can analyze images, process audio, and understand videos. Always provide accurate and helpful responses based on the content you receive."
  
  def load_model(self):
    """Load the model here and start the  server."""
    config_path = os.path.dirname(os.path.dirname(__file__))
    
    # Look for the custom chat template file in the same directory as the model
    template_file = os.path.join(os.path.dirname(__file__), "custom_chat_template.jinja")
    
    # Check if the chat template file exists
    if os.path.exists(template_file):
        chat_template = template_file
        logger.info(f"Using custom chat template: {chat_template}")
    else:
        chat_template = None
        logger.warning(f"Custom chat template file not found at {template_file}, using default template")
    
    # server args were generated by `upload` module
    # server_args = {'modalities': ['image', 'audio', 'video'], 'limit_mm_per_prompt': '{"image":10,"video":5,"audio":10}', 
    #                'max_model_len': '32000', 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 8192,
    #                'enable_chunked_prefill': True, 'reasoning_parser': None, 'enable_auto_tool_choice': False, 'tool_call_parser': None, 
    #                'dtype': 'auto', 'task': 'auto', 'kv_cache_dtype': 'fp8', 'tensor_parallel_size': 1, 'cpu_offload_gb': 0.0, 
    #                'quantization': None, 'port': 23333, 'host': 'localhost', 
    #                'checkpoints': 'model',
    #                #'additional_list_args': ['--max-num-seqs','16']
    #                }
    
    # # Only add chat_template if the file exists
    # if chat_template:
    #     server_args['chat_template'] = chat_template
    
    # print(f"DEBUG: server_args chat_template = {server_args.get('chat_template', 'Not set - using default')}")
    # # if checkpoints == "checkpoints" => assign to checkpoints var aka local checkpoints path
    # builder = ModelBuilder(config_path, download_validation_only=True)

    
    # stage = server_args.get("checkpoints")
    # if stage in ["build", "runtime"]:
    #   #checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")
    #   checkpoints = builder.download_checkpoints(stage=stage)
    #   server_args.update({"checkpoints": checkpoints})
      
    # if server_args.get("additional_list_args") == ['']:
    #   server_args.pop("additional_list_args")
    
    # modalities = server_args.pop("modalities", ["image", "audio", "video"])
    # if not modalities:
    #   modalities = ["image", "audio", "video"]
    # self.media_modalites = modalities

    # Start server
    # This line was generated by `upload` module
    #self.server = OpenAI_APIServer.from_vllm_backend(**server_args)

    base_url=f"http://0.0.0.0:23333"
    # self.watchdog = OpenAI_APIServer_Watchdog(
    #  base_url=base_url, health_check_interval=10,
    #  )
    
    self.client = OpenAI(
            api_key="notset",
            base_url=f"{base_url}/v1")
    self.model = self._get_model_id()
  
  # --------------------- Helper methods -------------------- #
  
  def _get_model_id(self):
    try:
      return self.client.models.list().data[0].id
    except Exception as e:
      raise ConnectionError("Failed to retrieve model ID from API") from e

  def _cl_custom_chat(
    self,
    prompt: str = "",
    images: List[Image] = [],
    audios: List[Audio] = [],
    videos: List[Video] = [],
    chat_history: List[Dict] = [],
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.8,
    stream=False,
    **completion_kwargs
  ) -> dict:
    """Process request through OpenAI API."""
    # Always use the default system prompt
    system_prompt = "" #self._load_default_system_prompt()
    
    openai_messages = build_messages(
        prompt, system_prompt, images, audios, videos, chat_history)

    if stream:
      # Force to use usage
      stream_options = completion_kwargs.pop("stream_options", {})
      stream_options.update({"include_usage": True})
      completion_kwargs["stream_options"] = stream_options
    
    if "tool_choice" in completion_kwargs and completion_kwargs.get("tool_choice") is None:
      completion_kwargs.pop("tool_choice", None)
    
    logger.info(f"additional completion_kwargs = {completion_kwargs}")
    
    response = self.client.chat.completions.create(
        model=self.model,
        messages=openai_messages,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        stream=stream,
        **completion_kwargs
      )

    return response
  
  # --------------------- Playground UI methods ------------------- # 
  
  @OpenAIModelClass.method
  def predict(
    self,
    prompt: str = "",
    images: List[Image] = [],
    audios: List[Audio] = [],
    videos: List[Video] = [],
    chat_history: List[Dict] = [],
    audio: Audio = None,
    video: Video = None,
    image: Image = None,
    tools: List[dict] = None,
    tool_choice: str = None,
    max_tokens: int = Param(default=2048, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),
    temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response.", ),
    top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", ),
    #reasoning_effort: str = Param(default="medium", description="The level of reasoning effort to apply to the response. Currently supported values are low, medium, and high. ", ),
  )-> str:
    """Method to call from UI
    """
    logger.info(f"Input: {prompt=}, {image=}")
    response = self._cl_custom_chat(
      prompt=prompt,
      images=[image] if image and (image.bytes or image.url) else images,
      audios=[audio] if audio and (audio.bytes or audio.url) else audios,
      videos=[video] if video and (video.bytes or video.url) else videos,
      chat_history=chat_history,
      max_tokens=max_tokens,
      temperature=temperature,
      top_p=top_p,
      stream=False,
      tool_choice=tool_choice,
      tools=tools,
      #reasoning_effort=reasoning_effort
    )
    
    self._set_usage(response)
    
    # If the response contains tool calls, return as a string
    if response.choices[0].message.tool_calls:
        tool_calls = response.choices[0].message.tool_calls
        tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)
        return tool_calls_json
    # Otherwise, return the content of the first choice
    else:
        content = response.choices[0].message.content
        return str(content) if content else ""

  @OpenAIModelClass.method
  def batch_predict(
    self,
    prompts: List[str],
    images: List[Image] = None,
    audios: List[Audio] = None,
    videos: List[Video] = None,
    chat_history: List[Dict] = [],
    tools: List[dict] = None,
    tool_choice: str = None,
    max_tokens: int = Param(
        default=2048,
        description="Maximum number of tokens to generate per request."
    ),
    temperature: float = Param(
        default=0.7,
        description="Randomness of the response."
    ),
    top_p: float = Param(
        default=0.95,
        description="Top‑p sampling parameter."
    ),
    max_concurrency: int = 32,
    max_batch_size: int = 16
) -> List[str]:
    """
    Run a batch of prompts (optionally with multimodal inputs) through the
    model concurrently, processing in chunks if batch size exceeds max_batch_size.

    The function returns a list of strings **in the same order** as the
    supplied ``prompts`` list.

    Parameters
    ----------
    prompts : List[str]
        Text prompts to be processed.
    images / audios / videos : Optional[List[Image|Audio|Video]]
        Lists whose length must equal ``len(prompts)``.  If ``None`` an empty
        list is used for every request.
    chat_history, tools, tool_choice, max_tokens, temperature, top_p :
        Same semantics as :meth:`predict`.
    max_concurrency :
        Upper bound for the number of parallel threads (default 32).
    max_batch_size :
        Maximum size for each batch chunk (default 16).

    Returns
    -------
    List[str]
        Model outputs (or tool‑call JSON) ordered to match ``prompts``.
    """

    batch_len = len(prompts)

    def _check_and_pad(arg, name):
        """
        Ensure ``arg`` is either ``None`` or a list of length ``batch_len``.
        Return a list of lists (each inner list may be empty) that can be
        indexed directly per prompt.
        """
        if arg is None:
            return [[] for _ in range(batch_len)]

        if not isinstance(arg, list):
            raise TypeError(f"{name} must be a list or None, got {type(arg)}")

        if len(arg) != batch_len:
            raise ValueError(
                f"The length of {name} ({len(arg)}) does not match the number of prompts ({batch_len})."
            )
        # Turn ``[img1, img2, …]`` into ``[[img1], [img2], …]``
        return [[item] if item is not None else [] for item in arg]

    img_batches = _check_and_pad(images, "images")
    aud_batches = _check_and_pad(audios, "audios")
    vid_batches = _check_and_pad(videos, "videos")

    def _run_one(idx: int, prompt: str, imgs: List, auds: List, vids: List) -> Dict[str, Any]:
        """
        Execute a single completion request and return a dict containing:
        - ``idx``   : original position in the batch
        - ``output``: the string that will be part of the final list
        - ``usage`` : the OpenAI usage object (may be ``None``)
        """
        try:
            response = self._cl_custom_chat(
                prompt=prompt,
                images=imgs,
                audios=auds,
                videos=vids,
                chat_history=chat_history,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stream=False,
                tool_choice=tool_choice,
                tools=tools,
            )

            usage = getattr(response, "usage", None)

            # ----------------------------------------------------------------
            # Turn the response into the same string format used by `predict`
            # ----------------------------------------------------------------
            if response.choices[0].message.tool_calls:
                tool_calls = response.choices[0].message.tool_calls
                output = json.dumps(
                    [tc.to_dict() for tc in tool_calls],
                    indent=2,
                )
            else:
                content = response.choices[0].message.content
                output = str(content) if content else ""

            return {"idx": idx, "output": output, "usage": usage}
        except Exception as exc:
            raise RuntimeError(f"Batch item {idx} raised: {exc}") from exc

    # ------------------------------------------------------------------
    # Helper to process a single chunk
    # ------------------------------------------------------------------
    def _process_chunk(
        chunk_prompts: List[str], 
        chunk_imgs: List[List], 
        chunk_auds: List[List], 
        chunk_vids: List[List],
        chunk_indices: List[int]
    ) -> List[Dict[str, Any]]:
        """Process a single chunk of data concurrently."""
        chunk_results = []
        chunk_size = len(chunk_prompts)
        
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=min(max_concurrency, chunk_size)
        ) as executor:
            futures = {
                executor.submit(
                    _run_one, 
                    chunk_indices[i], 
                    chunk_prompts[i], 
                    chunk_imgs[i], 
                    chunk_auds[i], 
                    chunk_vids[i]
                ): i for i in range(chunk_size)
            }
            
            for future in concurrent.futures.as_completed(futures):
                chunk_results.append(future.result())
        
        return chunk_results

    # ------------------------------------------------------------------
    # Split into chunks and process sequentially
    # ------------------------------------------------------------------
    if batch_len <= max_batch_size:
        # Single batch - process normally
        all_results = _process_chunk(
            prompts, 
            img_batches, 
            aud_batches, 
            vid_batches,
            list(range(batch_len))
        )
    else:
        # Multiple chunks - process sequentially
        all_results = []
        num_chunks = ceil(batch_len / max_batch_size)
        
        print(f"Processing {batch_len} items in {num_chunks} chunks of max size {max_batch_size}")
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * max_batch_size
            end_idx = min((chunk_idx + 1) * max_batch_size, batch_len)
            
            # Extract chunk data
            chunk_prompts = prompts[start_idx:end_idx]
            chunk_imgs = img_batches[start_idx:end_idx]
            chunk_auds = aud_batches[start_idx:end_idx]
            chunk_vids = vid_batches[start_idx:end_idx]
            chunk_indices = list(range(start_idx, end_idx))
            
            print(f"Processing chunk {chunk_idx + 1}/{num_chunks} (items {start_idx}-{end_idx-1})")
            
            # Process chunk
            chunk_results = _process_chunk(
                chunk_prompts, 
                chunk_imgs, 
                chunk_auds, 
                chunk_vids,
                chunk_indices
            )
            
            all_results.extend(chunk_results)

    # ------------------------------------------------------------------
    # Sort results back to the original order
    # ------------------------------------------------------------------
    all_results.sort(key=lambda x: x["idx"])
    ordered_outputs = [r["output"] for r in all_results]

    # ------------------------------------------------------------------
    # Aggregate usage information (mirrors the single‑call path)
    # ------------------------------------------------------------------
    total_usage = None
    for r in all_results:
        if r["usage"] is not None:
            if total_usage is None:
                total_usage = r["usage"]
            else:
                total_usage.prompt_tokens += r["usage"].prompt_tokens
                total_usage.completion_tokens += r["usage"].completion_tokens
                total_usage.total_tokens += r["usage"].total_tokens

    if total_usage is not None:
        class _FakeResp:
            usage = total_usage
        self._set_usage(_FakeResp())   # type: ignore[arg-type]

    print(f"{self._thread_local.token_contexts=}")
    return ordered_outputs


  @OpenAIModelClass.method
  def generate(
    self,
    prompt: str = "",
    images: List[Image] = [],
    audios: List[Audio] = [],
    videos: List[Video] = [],
    chat_history: List[Dict] = [],
    audio: Audio = None,
    video: Video = None,
    image: Image = None,
    tools: List[dict] = None,
    tool_choice: str = None,
    max_tokens: int = Param(default=2048, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),
    temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response.", ),
    top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", ),
    #reasoning_effort: str = Param(default="medium", description="The level of reasoning effort to apply to the response. Currently supported values are low, medium, and high. ", ),
  ) -> Iterator[str]:
    """Method to call generate from UI
    """
    logger.info(f"Input: {prompt=}, {image=}")
    stream_completion = self._cl_custom_chat(
        prompt=prompt,
        images=[image] if image and (image.bytes or image.url) else images,
        audios=[audio] if audio and (audio.bytes or audio.url) else audios,
        videos=[video] if video and (video.bytes or video.url) else videos,
        chat_history=chat_history,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True,
        tool_choice=tool_choice,
        tools=tools,
        #reasoning_effort=reasoning_effort
    )

    for chunk in stream_completion:
      self._set_usage(chunk)
      # If the response contains tool calls, return the first one as a string
      if chunk.choices:
        if chunk.choices[0].delta.tool_calls:
          tool_calls = chunk.choices[0].delta.tool_calls
          tool_calls_json = [tc.to_dict() for tc in tool_calls]
          # Convert to JSON string
          json_string = json.dumps(tool_calls_json, indent=2)
          # Yield the JSON string
          yield json_string
        else:
          content = chunk.choices[0].delta.content
          yield str(content) if content else ""
      else:
        yield ""
    
  
  def test(self):
    import requests
    
    const_data = dict(
      image=dict(
        images=[
          Image(url="https://samples.clarifai.com/metro-north.jpg")
        ],
        image=Image(url="https://samples.clarifai.com/metro-north.jpg")
      ),
      video=dict(videos=[
          Video(bytes=requests.get("https://samples.clarifai.com/beer.mp4").content)
      ]),
      audio=dict(audios=[
          Audio(
              url="https://samples.clarifai.com/GoodMorning.wav")
      ])
    )
    
    build_modalities_kwargs = {}
    for each in self.media_modalites:
      if each in const_data:
        build_modalities_kwargs.update(const_data.get(each))
    prompt = f"Describe this {', '.join(self.media_modalites)}" \
      if build_modalities_kwargs else "Hi there, how are you?"
    build_modalities_kwargs.update(dict(prompt=prompt))
  
    # dummy history
    chat_history = [
      dict(content="You're an assistant.", role="system"),
      dict(content="Hi.", role="user"),
      dict(content="Hi.", role="assistant"),
    ]
    # Test predict
    for k, v in build_modalities_kwargs.items():
      logger.info(f"# Media = {k}")
      logger.info("# 1/6: Test 'predict'.\n")
      print(self.predict(**{k:v}, max_tokens=512, temperature=0.9, top_p=0.9))
    
      logger.info("# 1.2/6: Test 'predict' History.\n")
      print(self.predict(**{k:v}, chat_history=chat_history, max_tokens=512, temperature=0.9, top_p=0.9))
      
      # Test generate
      logger.info("# 2/4: Test 'generate'.\n")
      for each in self.generate(**{k:v}, max_tokens=512, temperature=0.9, top_p=0.9):
        print(each, flush=True, end='')
      
      logger.info("# 2.2/6: Test 'generate' History.\n")
      for each in self.generate(**{k:v}, chat_history=chat_history, max_tokens=512, temperature=0.9, top_p=0.9):
        print(each, flush=True, end='')
    
    
    print("---"*5)
    print()
    logger.info("# 3/4: Test openai_transport")
    
    openai_request = {
      "messages": [
        {
          "role": "user",
          "content": "Hi, can you help me plan a trip to Japan?"
        },
        {
          "role": "assistant",
          "content": "Sure! When are you planning to go and what cities are you interested in visiting?"
        },
        {
          "role": "user",
          "content": "I’ll be there in October. I’d like to visit Tokyo, Kyoto, and Osaka."
        },
        {
          "role": "assistant",
          "content": "Great! October is a beautiful time in Japan. Would you prefer a cultural tour, food-focused trip, or something else?"
        },
        {
          "role": "user",
          "content": "I'm mostly interested in cultural sites and some local cuisine."
        }
      ],
      "temperature": 0.7,
      "top_p": 1.0,
      "n": 1,
      "max_tokens": 512,
    }
    #logger.info(f"{openai_request=}")
    string_request = json.dumps(openai_request)
    print(self.openai_transport(string_request))
    print("---"*5)
    
    logger.info("# 4/4: Test openai_stream_transport. Print out 5 chunks")
    openai_request["stream"] = True
    string_request = json.dumps(openai_request)
    iter_resp = self.openai_stream_transport(string_request)
    c = 0
    for each in iter_resp:
      print(each)
      c += 1
      if c > 5:
        break
    
    print("---"*5)
    
    logger.info("======= Benchmark single request ========")
    openai_request = {
      "messages": [
        {
          "role": "user",
          "content": "I want to go to Japan in this summer. Write a plan for me with more than 1000 words"
        },
      ],
      "temperature": 0.7,
      "top_p": 1.0,
      "max_tokens": 1024,
      "stream": True,
    }
    #logger.info(f"{openai_request=}")
    string_request = json.dumps(openai_request)
    iter_resp = self.openai_stream_transport(string_request)
    print("-------- Openai stream, max_tokens=1024")
    print(f"{openai_request=}")
    analyze_multiple_streams([iter_resp])
    
    print("-------- Generate, Multimodal Max Token : 1024")
    for k, v in build_modalities_kwargs.items():
      analyze_multiple_streams([self.generate(**{k:v}, chat_history=chat_history, max_tokens=1024, temperature=0.9, top_p=0.9)])
    
    print("-------- Generate, Max token : 1024")
    analyze_multiple_streams([self.generate(prompt="Write a 1000 word poem", chat_history=chat_history, max_tokens=1024, temperature=0.9, top_p=0.9)])
    
    print("-------- Generate, Max token : 1024, no history")
    analyze_multiple_streams([self.generate(prompt="Write a 1000 word poem", max_tokens=1024, temperature=0.9, top_p=0.9)])
